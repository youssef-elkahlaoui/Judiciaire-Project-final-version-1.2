import torch
from unsloth import FastLanguageModel
from transformers import AutoTokenizer

model_name = "ANASEEE/JudicIAreLLAMA"

try:
    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name = model_name,
        max_seq_length = 2048,
        dtype = torch.float16,
        load_in_4bit = True,
    )

    FastLanguageModel.for_inference(model)  # Required for Unsloth models
    print("âœ… Model and tokenizer loaded successfully")

except Exception as e:
    print(f"âŒ Failed to load model or tokenizer: {str(e)}")
    exit(1)

    
prompt = """### Instruction :
Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ù…ØªØ®ØµØµ ÙÙŠ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ† Ø§Ù„Ù…ØºØ±Ø¨ÙŠ.
Ø§Ù„Ø¬ÙˆØ§Ø¨ Ø®Ø§ØµÙˆ ÙŠÙƒÙˆÙ† ÙˆØ§Ø¶Ø­ØŒ Ù‚ØµÙŠØ±ØŒ ÙˆØ¨Ø§Ù„Ø¯Ø§Ø±Ø¬Ø© Ø§Ù„Ù…ØºØ±Ø¨ÙŠØ©ØŒ ÙˆÙ…Ø±ØªÙƒØ² ÙÙ‚Ø· Ø¹Ù„Ù‰ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†.

### Question :
Ø´Ù†Ùˆ Ù‡ÙŠ Ø¹Ù‚ÙˆØ¨Ø© Ø§Ù„Ø³Ø±Ù‚Ø© ÙØ§Ù„Ù‚Ø§Ù†ÙˆÙ† Ø§Ù„Ù…ØºØ±Ø¨ÙŠØŸ

### RÃ©ponse :
"""

inputs = tokenizer(prompt, return_tensors="pt").to("cuda")
outputs = model.generate(
    **inputs,
    max_new_tokens=256,
    temperature=0.7,
    top_p=0.9,
    do_sample=True,
    eos_token_id=tokenizer.eos_token_id,
)
response = tokenizer.decode(outputs[0], skip_special_tokens=True)
print("ğŸ“¤ Response:\n", response)
